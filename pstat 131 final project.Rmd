---
title: "pstat 131 final project"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


- For my project I decided to analyze my Spotify data. The dataset provided to me was a personalized data set given to my by Spotify after requesting it through my Settings" on the web platform. 


##Loading Packages
- First I downloaded the necessary packages, which were A LOT. 
```{r, include = FALSE}
library(rjson)
library(readr)
library(DBI)
library(viridis)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(plotly)
library(knitr)
library("gghighlight")
library(jsonlite)
library(dbplyr)
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(klaR) 
library(pROC)
library(glmnet)
library(dplyr)
library(janitor)
library(randomForest)
library(rpart)
library(rpart.plot)
library(ranger)
library(vip)
library(RJSONIO)
library(rjson)
tidymodels_prefer()
library(randomForest)
library(xgboost)
```


## Data
There were a lot of files provided to me through the zip provided (such as payment history, playlists, family plans, personal information) and I thought it would be best to work with the `StreamingHistory0.json` file, as it tracked every song I streamed, the artist, the milliseconds I streamed it, the date I streamed it. 

- I downloaded the JSON data provided to me into R. At first it was difficult to work with because it downloaded as a list. 


```{r}
#download streaming history from date
streaming_hist_0<-RJSONIO::fromJSON("/Users/ankitapattnaik/Downloads/MyData/StreamingHistory0.json", flatten=TRUE)
```


I then transformed the list into a dataframe using `sapply()` and turned my respective data to a data frame with 4 variables and 6,672 observations. 


```{r}
streaming_hist<-data.frame(t(sapply(streaming_hist_0,c)))

#check that the new created variable is a data.frame. it is. also check column name to see that the data frame categorized accordingly.
class(streaming_hist)
colnames(streaming_hist)
```


##Variable Transformation
Interpreting the `msPlayed`, or milliseconds played variable, was a little difficult so I transformed that data into seconds and minutes. I also took the `endTime` column and extracted the date to make a new variable with just data in the format of yyyy-mm-dd. That way my data would be easier to inetrpret. 

I additionally created a `skipped` variable which determined whether or not I skipped the data based off the now minutes played. If I listened to the song for less than a minute and a half, I assumed I skipped the song and if it was more than one minute and thirty seconds, I assumed I streamed it. I based this off the google result for the timing of an average song which is: three minutes and thirty seconds.

I then called `clean_names()` function to the data to make the variable names easier to work with and unlisted the variables into characters, as the observations of each data set seemed to have stored into my data frame as lists. 


```{r}
spotify <- streaming_hist %>% 
  as_tibble() %>% 
  mutate_at("endTime", ymd_hm) %>% 
  mutate(endTime = endTime - hours(6)) %>% 
  mutate(date = floor_date(endTime, "day") %>% as_date, seconds = as.numeric(msPlayed) / 1000, minutes = seconds / 60) %>%
  mutate(skipped= case_when(
    minutes > 1.5 ~ "streamed",
    minutes < 1.5 ~ "skipped"
  )) %>%
  mutate(skipped = factor(skipped, levels = c("skipped", "streamed")))

spotify<-clean_names(spotify)

#turn remainder of the of the listed variables into characters
spotify$artist_name<-unlist(spotify$artist_name)
spotify$track_name<-unlist(spotify$track_name)
spotify$ms_played<-unlist(spotify$ms_played)


```

##EDA: Exploratory Data Analysis 

I had a lot of fun making these graphs. I found an online source that guided me through seeing how I could manipulate my variables to display specifically Spotify data and I had a lot of fun playing around seeing my streaming history. 


```{r}
#playback activity
streamingHours <- spotify %>% 
  filter(date >= "2020-01-01") %>% 
  group_by(date) %>% 
  group_by(date = floor_date(date, "week")) %>%
  summarize(hours = sum(minutes) / 60) %>% 
  arrange(date) %>% 
  ggplot(aes(x = date, y = hours)) + 
  geom_col(aes(fill = hours)) +
  scale_fill_gradient(low = "yellow", high = "red") + 
  labs(x= "Date", y= "Hours of music playback") + 
  ggtitle("Playback Activity by Week")
streamingHours


#playback activity by artist
hoursArtist <- spotify %>% 
  group_by(artist_name, date = floor_date(date, "month")) %>% 
  summarize(hours = sum(minutes) / 60) %>% 
  ggplot(aes(x = date, y = hours, group = artist_name)) + 
  labs(x= "Date", y= "Hours of Music") + 
  ggtitle("Most Listened to Artists", "My top 3: DAY6, NCT DREAM, and SEVENTEEN") +
  geom_line() + 
  gghighlight(artist_name == "DAY6" || artist_name == "NCT DREAM" || artist_name == "SEVENTEEN") 
hoursArtist

#top 10 most streamed tracks bar graph
top10_track<-spotify %>%
  count(spotify$track_name) %>%
  arrange(-n)%>%
  head(.,10)
names(top10_track)<-c('track', 'counter')

mostStreamed <- ggplot(data = top10_track, aes(track, count)) +
  geom_bar(stat = "identity", width = 0.7, fill = "pink")+
  theme_minimal()+
  geom_text(aes(label = count), hjust = 0.5, vjust = 0.7)+
  labs(title = "Top 10 Most Streamed Songs", x = "Songs", y = "Streams")+
  theme(axis.text.x = element_text(angle = 40, size = 7))
mostStreamed

#how many hours of kpop, by genre
k_music <- spotify %>% 
  group_by(artist_name, date = floor_date(date, "hour")) %>% 
  mutate(artist_name = iconv(artist_name, to = "UTF-8")) %>% 
  filter(artist_name == "DAY6" ||
         artist_name == "NCT DREAM" ||
         artist_name == "SEVENTEEN" ||
         artist_name == "BTS" ||
         artist_name == "DAY6 (Even of Day)" ||
         artist_name == "Sam Kim" ||
         artist_name == "TOMORROW X TOGETHER" ||
         artist_name == "JANNABI" ||
         artist_name == "AKMU" ||
         artist_name == "Young K" ||
         artist_name == "Zion.T") %>% 
  summarize(hours = sum(minutes)/60) %>% 
  ggplot(aes(x= artist_name, y = hours)) + 
  geom_col(aes(fill = artist_name))+
  scale_fill_viridis(discrete = TRUE,guide = FALSE, option="D") +
  labs(title = "Playback by artist: Korean Music") +
  theme_light(base_size=12) +
  xlab("") +
  coord_flip()
k_music

top10_artist<-spotify %>%
  count(spotify$artist_name) %>%
  arrange(-n)%>%
  head(.,10)
names(top10_artist)<-c('artist name', 'count')
top10_artist


# Listened time vs. number of songs
listenedTimes_songs <- top10_track %>% 
  group_by(track) %>% 
  mutate(track = iconv(track, to = "UTF-8")) %>% 
  summarize(timesStreamed = sum(counter), total_songs= n_distinct(timesStreamed)) %>%
  arrange(desc(timesStreamed)) %>% 
  slice(1:10) %>% 
  ggplot(aes(x=track , y = timesStreamed,color = track)) +
  geom_point(size = 3, shape = 15) +
  scale_color_viridis(discrete = TRUE, option="C") +
  theme_light(base_size=10, base_family="HiraKakuProN-W3") +
  theme (
    axis.text.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank(),
  ) +
  labs(title = "Relationship between number of songs and listened time") +
  xlab("Tracks")+
  ylab("Streams")
listenedTimes_songs
   
  
#listening to spotify throughout the week
dayHour <- spotify %>% 
  group_by(date, hour = hour(end_time), weekday = wday(date, label = TRUE)) %>% 
  summarize(hoursListened = sum(minutes)/60)

dayHour %>% 
  group_by(weekday, hour) %>% 
  summarize(hours = sum(hoursListened)) %>% 
  ggplot(aes(x= hour, y= weekday, fill= hours)) +
  geom_tile() +
  scale_x_continuous(breaks= seq(0, 24, 2)) +
  scale_fill_gradient(low= "darkolivegreen1", high= "cyan4") +
  labs(title= "Average Spotify time during the week", 
       x= "Hour", y= "") +
  theme_light()
```









```{r}
#decide which artists to use for my data because there are too many artists I listen to. 
spotify %>%
  count(artist_name) %>%
  arrange(-n)%>%
  head(.,20)
  
spotify %>%
  count(track_name) %>%
  arrange(-n)%>%
  head(.,20)
#DAY6, NCT DREAM, SEVENTEEN, Rex Orange County, BTS, Honne, DAY6 (Even of Day), Sam Kim, Ravi Shankar, Tomorrow x Together, Kanye West, JANNABI, Doja Cat, AKMU, Young K, Zion. T, Tyler, The Creator, DJ Khaled, Lorde, GOT7.
set.seed(2222)
#begin to subset the data with my top 10 artists
spotify <- spotify %>%
  filter(artist_name == "DAY6" | artist_name == "NCT DREAM" | artist_name == "SEVENTEEN" | artist_name ==  "Rex Orange County" | artist_name ==  "BTS" | artist_name == "HONNE"| artist_name == "DAY6 (Even of Day)"|artist_name == "Sam Kim"|artist_name == "Ravi Shankar"|artist_name == "TOMORROW X TOGETHER"|artist_name == "Kanye West"|artist_name == "JANNABI"|artist_name == "Doja Cat"|artist_name == "AKMU"|artist_name == "Young K"|artist_name == "Zion.T"|artist_name == "Tyler, The Creator"|artist_name == "DJ Khaled"|artist_name == "Lorde"|artist_name == "GOT7")

#subset the data, split

#get rid of end_time and ms_played because they are useless to us, we converted ms_played to a more easily interpreted value: seconds and minutes
spotify$end_time<-NULL
spotify$ms_played<-NULL

#double check that there is a good spread of skipped and unskipped songs. it is about a 63:37 split, pretty good. 
spotify %>%
  count(skipped) %>%
  arrange(-n)

spotify_split <- initial_split(spotify, strata = skipped, prop = 0.7)
class(spotify_split) #check that the data has split

spotify_train <- training(spotify_split)
spotify_test <- testing(spotify_split)
dim(spotify_train)

#factorize all categorical values
spotify_train$artist_name<-as.factor(spotify_train$artist_name)
spotify_train$track_name<-as.factor(spotify_train$track_name)
spotify_train$date<-as.factor(spotify_train$date)
spotify_train$skipped<-as.factor(spotify_train$skipped)
#with 2,000+ observations, we will use the train data split as our main data.
```

```{r}
# recipe

#get rid of end_time column, it is useless to us

spotify_recipe <- recipe(skipped ~ artist_name+ track_name + date                          +seconds+minutes,
                         data = spotify_train) %>%
                  step_dummy(all_nominal(), -all_outcomes())%>%
                  step_normalize(all_predictors(), -all_outcomes())%>%
                  step_nzv(all_predictors())


spotify_recipe %>% prep() %>% juice()


#random forest or boosted tree model 
```


```{r}
#fitting classification trees
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")
class_tree_fit <- class_tree_spec %>%
  fit(skipped ~ ., data = spotify_train)
#to implement a little of what we learned LOL this classification decision tree is a LOL though. really put into prespective how my skipped and streamed variables go though :) 
class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

#check training set accuracy 
augment(class_tree_fit, new_data = spotify_train) %>%
  accuracy(truth = skipped, estimate = .pred_class)
#very accurate

#check confusion matrix
augment(class_tree_fit, new_data = spotify_train) %>%
  conf_mat(truth = skipped, estimate = .pred_class)

#check with testing data
#first factor all testing data, because we only factored trainig
spotify_test$artist_name<-as.factor(spotify_test$artist_name)
spotify_test$track_name<-as.factor(spotify_test$track_name)
spotify_test$date<-as.factor(spotify_test$date)
spotify_test$skipped<-as.factor(spotify_test$skipped)

#then check
augment(class_tree_fit, new_data = spotify_test) %>%
  conf_mat(truth = skipped, estimate = .pred_class)
augment(class_tree_fit, new_data = spotify_test) %>%
  accuracy(truth = skipped, estimate = .pred_class)


###############3
```

```{r}
set.seed(2435)
spotify_fold <- vfold_cv(spotify_train, strata = skipped, v = 10, repeats = 2, na.rm= TRUE)

#classification tree 
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification")
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(spotify_recipe)

###  accuracy grid
param_grid1 <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res_accuracy <- tune_grid(
  class_tree_wf, 
  resamples = spotify_fold, 
  grid = param_grid1, 
  metrics = metric_set(accuracy)
)
autoplot(tune_res_accuracy)
#we could have predicted this from our decision tree because it was either 1 or 0. 

```

```{r}
### fitting and tuning an elastic net
elastic_net_spec <- multinom_reg(penalty = tune(), 
                                 mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

en_workflow <- workflow() %>% 
  add_recipe(spotify_recipe) %>% 
  add_model(elastic_net_spec)

en_grid <- grid_regular(penalty(range = c(-5, 5)), 
                        mixture(range = c(0, 1)), levels = 10)
#fit models to folded data
tune_res_en <- tune_grid(
  en_workflow,
  resamples = spotify_fold, 
  grid = en_grid
)

autoplot(tune_res_en)

 collect_metrics(tune_res_en)%>%
  arrange(-mean)
```

```{r}
 #random forest
 rf_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
rf_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(spotify_recipe)

param_grid2 <- grid_regular(mtry(range = c(1, 8)),trees(range = c(50,300)),min_n(range=c(1,5)), levels = 8)
tune_res_rf <- tune_grid(
  rf_wf,
  resamples = spotify_fold,
  grid = param_grid2,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_rf)
```

```{r}
#boosted model
bt_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")
bt_wf <- workflow() %>%
  add_model(bt_spec %>% set_args(trees = tune())) %>%
  add_recipe(spotify_recipe)
param_grid3 <- grid_regular(trees(range = c(10,2000)), levels = 10)
tune_res_bt <- tune_grid(
  bt_wf,
  resamples = spotify_fold,
  grid = param_grid3,
  metrics = metric_set(roc_auc)
)
autoplot(tune_res_bt)

#we can see the details of the plot here
collect_metrics(tune_res_bt)%>%
  arrange(-mean)

#compare between boosted tree and random forest


```



